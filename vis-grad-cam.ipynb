{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8bec7-2ada-4326-8b82-009a3abafe31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir --parents /tmp/cache/yaak-datasets/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40c075-4f54-4e9a-a642-98e663fe19b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import bertviz\n",
    "import pytorch_grad_cam\n",
    "from hydra.utils import instantiate\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import more_itertools as mit\n",
    "from deephouse.tools.camera import Camera\n",
    "from einops import rearrange, reduce, repeat\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "\n",
    "class Unnormalize(Normalize):\n",
    "    def __init__(self, mean, std, **kwargs):\n",
    "        mean = torch.tensor(mean)\n",
    "        std = torch.tensor(std)\n",
    "\n",
    "        super().__init__(\n",
    "            mean=(-mean / std).tolist(),\n",
    "            std=(1.0 / std).tolist(),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "def get_unnorm_frames(batch, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):    \n",
    "    clips = mit.one(batch[\"clips\"].values())\n",
    "    frames = clips[\"frames\"][:,-1,...]\n",
    "\n",
    "    unorm = Unnormalize(mean, std)\n",
    "    imgs = unorm(frames)\n",
    "    return imgs.clamp(min=0.0, max=1.0)\n",
    "\n",
    "\n",
    "def get_frames(batch):\n",
    "    # Return the last frame from the context\n",
    "    clips = mit.one(batch[\"clips\"].values())\n",
    "    frames = clips[\"frames\"][:,-1,...]\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a1459-27b9-4533-91c7-02ce898e69d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cargpt.models.cilpp import CILpp\n",
    "\n",
    "\n",
    "# wandb_model = \"yaak/cargpt/model-knmg1pgn:v0\"\n",
    "wandb_model = \"yaak/cargpt/model-6hm5s715:v0\"\n",
    "cilpp = CILpp.load_from_wandb_artifact(name=wandb_model)\n",
    "# cilpp = CILpp.load_from_checkpoint(\"artifacts/model-rc93mcrx:v7/model.ckpt\")\n",
    "# cilpp.eval()\n",
    "\n",
    "# cfg = OmegaConf.load(\"config/experiment/cilpp.yaml\")\n",
    "# datamodule = instantiate(cfg.datamodule)\n",
    "# data = datamodule.val_dataloader()\n",
    "\n",
    "cfg = OmegaConf.load(\"config/data/train.yaml\")\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "data = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3543bc-3e24-42a2-b40b-77e103d38975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c9ec6-3684-4b32-9488-67bd0f11bbcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "imgs = rearrange(get_unnorm_frames(batch), 'b c h w -> b h w c')\n",
    "print(imgs.shape)\n",
    "plt.imshow(imgs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41f796-9286-494b-9712-f08ca35f662b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Try with the simples classification\n",
    "\n",
    "Based on https://jacobgil.github.io/pytorch-gradcam-book/introduction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab24e1a-2051-4d55-ae30-38dc9a766f57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet34, resnet50\n",
    "\n",
    "model1 = resnet34(weights=\"IMAGENET1K_V1\")\n",
    "target_layers = [model1.layer4[-1]]\n",
    "\n",
    "idx = 2\n",
    "input_tensor = get_frames(batch)[idx:idx+1]\n",
    "\n",
    "def draw_only_resnet_vis(cam_cls, tgt_cls_int, title):\n",
    "    with cam_cls(model=model1, target_layers=target_layers, use_cuda=True) as cam:\n",
    "        targets = [ClassifierOutputTarget(tgt_cls_int)]\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    visualization = show_cam_on_image(imgs[idx].numpy(), grayscale_cam, use_rgb=True)\n",
    "    plt.imshow(visualization);\n",
    "    plt.title(title);\n",
    "    plt.show()\n",
    "\n",
    "plt.close()\n",
    "for cam_cls in [GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad]:\n",
    "    print(cam_cls.__name__)\n",
    "    # draw_only_resnet_vis(cam_cls, tgt_cls_int=479, title=f\"{cam_cls.__name__}: car wheel\")\n",
    "    # draw_only_resnet_vis(cam_cls, tgt_cls_int=475, title=f\"{cam_cls.__name__}: car mirror\")\n",
    "    draw_only_resnet_vis(cam_cls, tgt_cls_int=920, title=f\"{cam_cls.__name__}: traffic light, traffic signal, stoplight\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17b79f-dcc4-429d-9232-808f99d5a32e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Try with resnet from our architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45c8c9-7d0d-4d59-af63-2401c36b72a6",
   "metadata": {},
   "source": [
    "The results are not the same due to **running mean and average of Batch Normalization** layers! All the weights are the same between ResNet34 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af86587-730b-47b6-bf36-493d31cfacdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = next(cilpp.state_embedding.modules())['frame']['backbone'].resnet\n",
    "model2.requires_grad_(True)\n",
    "target_layers = [model2.layer4[-1]]\n",
    "\n",
    "idx = 2\n",
    "input_tensor = get_frames(batch)[idx:idx+1]\n",
    "\n",
    "def draw_only_resnet_vis(cam_cls, tgt_cls_int, title):\n",
    "    with cam_cls(model=model2, target_layers=target_layers, use_cuda=True) as cam:\n",
    "        targets = [ClassifierOutputTarget(tgt_cls_int)]\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    visualization = show_cam_on_image(imgs[idx].numpy(), grayscale_cam, use_rgb=True)\n",
    "    plt.imshow(visualization);\n",
    "    plt.title(title);\n",
    "    plt.show()\n",
    "\n",
    "plt.close()\n",
    "for cam_cls in [GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad]:\n",
    "    # draw_only_resnet_vis(cam_cls, tgt_cls_int=479, title=f\"{cam_cls.__name__}: car wheel\")\n",
    "    # draw_only_resnet_vis(cam_cls, tgt_cls_int=475, title=f\"{cam_cls.__name__}: car mirror\")\n",
    "    draw_only_resnet_vis(cam_cls, tgt_cls_int=920, title=f\"{cam_cls.__name__}: traffic light, traffic signal, stoplight\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9081be6-850d-4334-b4bb-4cd3b2c6bd2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Naive try with the whole CIL++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500bb15-dc6a-4057-8f18-07166c23cf88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from einops import rearrange, reduce, repeat\n",
    "from jaxtyping import Float, Shaped\n",
    "from torch import Tensor\n",
    "\n",
    "import numpy as np\n",
    "from pytorch_grad_cam.base_cam import BaseCAM\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, RawScoresOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet34, resnet50\n",
    "\n",
    "class WrapperModel(pl.LightningModule):\n",
    "    def __init__(self, cilpp):\n",
    "        super().__init__()\n",
    "        self.cilpp = cilpp\n",
    "        self._camera = None\n",
    "\n",
    "        cilpp.requires_grad_(True)\n",
    "        cilpp.eval()\n",
    "\n",
    "        # self.cuda()\n",
    "        self.requires_grad_(True)\n",
    "        self.eval()\n",
    "\n",
    "    def prepare_input_tensor(self, batch):\n",
    "        clips = mit.one(batch[\"clips\"].values())\n",
    "        frames = clips[\"frames\"]\n",
    "        speed = clips[\"meta\"][\"VehicleMotion_speed\"].to(frames)\n",
    "\n",
    "        if isinstance(_camera_params := clips.get(\"camera_params\", {}).copy(), dict):\n",
    "            camera_params = _camera_params.copy()\n",
    "            camera_model = mit.one(set(camera_params.pop(\"model\")))\n",
    "            _, t, *_ = frames.shape\n",
    "            # need one camera per frame\n",
    "            camera_params = {\n",
    "                k: repeat(v, \"b -> (b t)\", t=t) for k, v in camera_params.items()\n",
    "            }\n",
    "            camera = Camera.from_params(model=camera_model, params=camera_params)\n",
    "            camera = camera.to(frames)\n",
    "        else:\n",
    "            camera = None\n",
    "\n",
    "        self._camera = camera\n",
    "\n",
    "        b, t, c, h, w = frames.shape\n",
    "        speed = repeat(speed, 'b sc -> b t c h w sc', t=t, c=c, h=h, w=w)\n",
    "        frames = rearrange(frames, \"b t c h (w fc) -> b t c h w fc\", fc=1)\n",
    "        input_tensor = torch.concat([frames, speed], dim=-1)\n",
    "        input_tensor = rearrange(input_tensor, \"b t c h w A -> b A t c h w\")\n",
    "        return input_tensor\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor = rearrange(input_tensor, \"b A t c h w -> b t c h w A\")\n",
    "        frames = input_tensor[..., 0]\n",
    "        speed = input_tensor[:, 0, 0, 0, 0, 1:]\n",
    "        camera = self._camera\n",
    "        pred = self.cilpp(frames=frames, speed=speed, camera=camera)\n",
    "        pred = rearrange(pred, \"b 1 c -> b c\")\n",
    "        return pred\n",
    "\n",
    "    \n",
    "class NegPosGradCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers, use_cuda=False,\n",
    "                 reshape_transform=None):\n",
    "        super().__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            use_cuda,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_layers,\n",
    "                        targets,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_tensor.detach())  # [B, 2]\n",
    "            if outputs.shape[0] > 1: raise NotImplementedError(\"Only batch size == 1 for now\")\n",
    "            scalars_outputs = [target(output)\n",
    "                       for target, output in zip(targets, outputs)]\n",
    "        grads = grads.copy()\n",
    "        out = scalars_outputs[0].item()\n",
    "        print(\"O\", outputs, \"SO\",scalars_outputs, \"out\", out)\n",
    "        if out < 0:\n",
    "            elems = np.sum(grads < 0, axis=(2,3))\n",
    "            selected_grads = -1.0 * np.where(grads < 0, grads, 0)\n",
    "        else:\n",
    "            elems = np.sum(grads >= 0, axis=(2,3))\n",
    "            selected_grads = np.where(grads >= 0, grads, 0)\n",
    "        # print(np.sum(selected_grads, axis=(2, 3)), elems)\n",
    "        return np.sum(selected_grads, axis=(2, 3)) / (elems + 1e-8)\n",
    "\n",
    "    \n",
    "class RawAccelerationTarget:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        # print(model_output)\n",
    "        return model_output[0]\n",
    "\n",
    "\n",
    "class RawSteeringTarget:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        # print(model_output)\n",
    "        return model_output[1]\n",
    "\n",
    "\n",
    "def plot_images(images_dict, size=4):\n",
    "    outer_keys = list(images_dict.keys())\n",
    "    n_outer = len(outer_keys)\n",
    "    n_inner = max(len(inner_dict) for inner_dict in images_dict.values())\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_inner, ncols=n_outer, figsize=(int(size*1.5 * n_outer), size * n_inner))\n",
    "\n",
    "    for i, outer_key in enumerate(outer_keys):\n",
    "        inner_dict = images_dict[outer_key]\n",
    "\n",
    "        for j, inner_key in enumerate(inner_dict):\n",
    "            img = inner_dict[inner_key]\n",
    "            ax = axes[j, i] if n_inner > 1 else axes[i]\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f'{outer_key} - {inner_key}')\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e052e7a-f7e7-4cb4-8262-b3628cdf74f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idx = 1 has positive outcomes, idx=2 negative\n",
    "# idx = 2\n",
    "def vis_batch_idx(batch, idx, imgs):\n",
    "    wrapper = WrapperModel(cilpp)\n",
    "    target_layers = [next(cilpp.state_embedding.modules())['frame']['backbone'].resnet.layer4[-1]]\n",
    "\n",
    "    input_tensor = wrapper.prepare_input_tensor(batch)[idx: idx+1]\n",
    "\n",
    "    def get_vis(cam_cls, title, targets):\n",
    "        with cam_cls(model=wrapper, target_layers=target_layers, use_cuda=True) as cam:\n",
    "            grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        visualization = show_cam_on_image(imgs[idx].numpy(), grayscale_cam, use_rgb=True)\n",
    "        return visualization\n",
    "\n",
    "    plt.close()\n",
    "    classes = [\n",
    "               # GradCAMPlusPlus, \n",
    "               # GradCAM, \n",
    "               NegPosGradCAM,\n",
    "               # HiResCAM, \n",
    "               #ScoreCAM, \n",
    "               #AblationCAM, \n",
    "               # XGradCAM, \n",
    "               # EigenCAM, \n",
    "               #FullGrad,\n",
    "              ]\n",
    "    to_plot = {}\n",
    "    for target_cls in [RawAccelerationTarget, RawSteeringTarget]:\n",
    "        target_to_plot = to_plot.setdefault(target_cls.__name__, {})\n",
    "        for cam_cls in classes:\n",
    "            try:\n",
    "                target_to_plot[cam_cls.__name__] = get_vis(cam_cls, targets=[target_cls()], title=f\"{cam_cls.__name__}: {target_cls.__name__}\")\n",
    "            except (RuntimeError, AttributeError) as exc:\n",
    "                print(f\"Can't do {cam_cls.__name__}: {target_cls.__name__}\")\n",
    "                print(exc)\n",
    "            finally:\n",
    "                print(\"-\"*60)\n",
    "    \n",
    "    plot_images(to_plot, 4)\n",
    "    print((to_plot[RawAccelerationTarget.__name__][NegPosGradCAM.__name__] == to_plot[RawSteeringTarget.__name__][NegPosGradCAM.__name__]).sum(), to_plot[RawAccelerationTarget.__name__][NegPosGradCAM.__name__].shape)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(imgs.shape)\n",
    "for idx in range(imgs.shape[0]):\n",
    "    print(\">>>\", idx)\n",
    "    vis_batch_idx(batch, idx, imgs)\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604f73e-3c78-4bea-a97b-f1406949aabf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e757a8-c213-436e-bf51-79ceab2cc7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
