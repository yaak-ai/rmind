{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488fc1e-7375-4e1c-b007-a05bfd258d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir --parents /tmp/cache/yaak-datasets/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd481b-b78e-4303-9622-9fefd2065a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import bertviz\n",
    "import pytorch_grad_cam\n",
    "from hydra.utils import instantiate\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import more_itertools as mit\n",
    "from deephouse.tools.camera import Camera\n",
    "from einops import rearrange, reduce, repeat\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "\n",
    "class Unnormalize(Normalize):\n",
    "    def __init__(self, mean, std, **kwargs):\n",
    "        mean = torch.tensor(mean)\n",
    "        std = torch.tensor(std)\n",
    "\n",
    "        super().__init__(\n",
    "            mean=(-mean / std).tolist(),\n",
    "            std=(1.0 / std).tolist(),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        \n",
    "def get_figure(images, batch_no):\n",
    "    layers, heads, height, width, _ = images.shape\n",
    "\n",
    "    # determine the size of the figure based on the aspect ratio of the images\n",
    "    base_size = 12\n",
    "    if width > height:\n",
    "        fig_width = base_size\n",
    "        fig_height = base_size * height / width\n",
    "    else:\n",
    "        fig_height = base_size\n",
    "        fig_width = base_size * width / height\n",
    "        \n",
    "    # create a new figure with 4x4 subplots\n",
    "    fig, axs = plt.subplots(layers, heads, figsize=(fig_width, fig_height), gridspec_kw={'wspace': 0, 'hspace': 0})\n",
    "    \n",
    "    # add a border between subplots\n",
    "    spine_kwargs = {'color': 'white', 'linewidth': 1.}\n",
    "    for ax in axs.flat:\n",
    "        ax.spines['top'].set(**spine_kwargs)\n",
    "        ax.spines['bottom'].set(**spine_kwargs)\n",
    "        ax.spines['left'].set(**spine_kwargs)\n",
    "        ax.spines['right'].set(**spine_kwargs)\n",
    "        ax.tick_params(axis='both', which='both', length=0, labelbottom=False, labelleft=False)\n",
    "        \n",
    "    fig.suptitle(f\"Batch[{batch_no}]\", fontsize=14)\n",
    "    fig.text(0.5, 0.05, \"Heads\" if heads > 1 else \"Avg heads\", ha='center', fontsize=10)\n",
    "    fig.text(0.1, 0.5, \"Layers\", va='center', rotation='vertical', fontsize=10)\n",
    "\n",
    "    # loop through each subplot and display the corresponding image\n",
    "    for i in range(layers):\n",
    "        for j in range(heads):\n",
    "            img = images[i, j]\n",
    "            try:\n",
    "                ax = axs[i, j]\n",
    "            except IndexError:\n",
    "                ax = axs[i]\n",
    "            ax.imshow(img)\n",
    "            if j==0:\n",
    "                ax.set_ylabel(i, rotation=0)\n",
    "                ax.yaxis.set_label_coords(-0.1,0.5)\n",
    "            if i==(layers-1):\n",
    "                ax.set_xlabel(j)\n",
    "\n",
    "    return fig\n",
    "        \n",
    "    \n",
    "@torch.no_grad()\n",
    "def get_state_and_frames(cilpp, batch):\n",
    "    clips = mit.one(batch[\"clips\"].values())\n",
    "    frames = rearrange(clips[\"frames\"], \"b 1 c h w -> b c h w\")\n",
    "\n",
    "    meta = clips[\"meta\"]\n",
    "    speed = meta[\"VehicleMotion_speed\"].to(torch.float32)\n",
    "    \n",
    "    if any(camera_params := clips.get(\"camera_params\", {}).copy()):\n",
    "        camera_model = mit.one(set(camera_params.pop(\"model\")))\n",
    "        camera = Camera.from_params(model=camera_model, params=camera_params)\n",
    "        camera = camera.to(frames)\n",
    "    else:\n",
    "        camera = None\n",
    "    \n",
    "    state = cilpp._embed_state(frames=frames, speed=speed, camera=camera)\n",
    "    \n",
    "    return state, frames\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_labels(batch):\n",
    "    clips = mit.one(batch[\"clips\"].values())\n",
    "    meta = clips[\"meta\"]\n",
    "\n",
    "    gas = meta[\"VehicleMotion_gas_pedal_normalized\"]\n",
    "    brake = meta[\"VehicleMotion_brake_pedal_normalized\"]\n",
    "    # NOTE: assuming (gas > 0) xor (brake > 0)\n",
    "    accel_lbl = gas - brake\n",
    "    steering_lbl = meta[\"VehicleMotion_steering_angle_normalized\"]\n",
    "\n",
    "    return {\n",
    "        \"acceleration\": accel_lbl.to(torch.float32),\n",
    "        \"steering_angle\": steering_lbl.to(torch.float32),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_attention_maps(encoder: torch.nn.modules.transformer.TransformerEncoder, x: torch.Tensor, mask=None):\n",
    "    attention_maps = []\n",
    "    for l in encoder.layers:\n",
    "        attn_x = x.detach()\n",
    "        _, attn_map = l.self_attn(attn_x, attn_x, attn_x, attn_mask=mask, need_weights=True, average_attn_weights=False)\n",
    "        attention_maps.append(attn_map)\n",
    "        x = l.forward(x)\n",
    "    return attention_maps\n",
    "\n",
    "\n",
    "def merge_frames_with_maps(frames, maps, map_weight=0.5, boost_channel=0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), out_height=40, \n",
    "                           norm_maps=False, global_norm=True, no_attn_itself=True, avg_heads=False):\n",
    "    batch_size, layers, heads, tokens, _= maps.shape\n",
    "    _, channels, height, width = frames.shape\n",
    "    \n",
    "    out_scale_factor = out_height / height\n",
    "    \n",
    "    if no_attn_itself:\n",
    "        tmp = torch.eye(tokens, tokens) * (-20000) # same effect as -inf, but avoiding 0*(-inf) = nan\n",
    "        tmp = repeat(tmp, 't1 t2 -> b layers heads t1 t2', b=batch_size, layers=layers, heads=heads)\n",
    "        maps = maps * tmp\n",
    "    maps = torch.nn.functional.softmax(maps, dim=-1)\n",
    "    maps = torch.mean(maps, dim=-2)  # batch_size, layers, heads, tokens\n",
    "    \n",
    "    if avg_heads:\n",
    "        maps = maps.mean(dim=2, keepdim=True)\n",
    "        heads = 1\n",
    "    \n",
    "    if norm_maps:\n",
    "        if global_norm:\n",
    "            tokens_min = reduce(maps, 'b layers heads tokens -> b', \"min\")\n",
    "            tokens_max = reduce(maps, 'b layers heads tokens -> b', \"max\")\n",
    "            tokens_min = repeat(tokens_min, 'b -> b layers heads tokens', layers=layers, heads=heads, tokens=tokens)\n",
    "            tokens_max = repeat(tokens_max, 'b -> b layers heads tokens', layers=layers, heads=heads, tokens=tokens)\n",
    "        else:\n",
    "            tokens_min, _ = maps.min(dim=-1, keepdim=True)\n",
    "            tokens_max, _ = maps.max(dim=-1, keepdim=True)\n",
    "        maps = (maps - tokens_min) / (tokens_max - tokens_min)\n",
    "    \n",
    "    R = width / height\n",
    "    H = (tokens / R) ** 0.5\n",
    "    maps = rearrange(maps, 'b layers (heads c) (H W) -> (b layers heads) c H W', H=int(H), c=1)\n",
    "        \n",
    "    maps = torch.nn.functional.interpolate(maps, scale_factor=height/H*out_scale_factor, mode='nearest-exact')\n",
    "    maps = rearrange(maps, '(b layers heads) c H W -> b layers (heads c) H W', layers=layers, heads=heads)\n",
    "    \n",
    "    unorm = Unnormalize(mean, std)\n",
    "    imgs = unorm(frames)\n",
    "    imgs = torch.nn.functional.interpolate(imgs, scale_factor=out_scale_factor, mode='bicubic')\n",
    "    imgs = imgs.clamp(min=0., max=1.)\n",
    "    \n",
    "    imgs = repeat(imgs, 'B C H W -> B layers heads C H W', layers=layers, heads=heads)\n",
    "    imgs = rearrange(imgs, 'B layers heads C H W -> C B layers heads H W').clone()\n",
    "    if boost_channel < 3:\n",
    "        imgs[boost_channel] = (1. - map_weight) * imgs[boost_channel] + map_weight * maps\n",
    "    else:\n",
    "        if norm_maps:\n",
    "            maps = (1. - map_weight) + map_weight * maps\n",
    "        imgs = torch.concatenate([imgs, rearrange(maps, '(C b) layers heads H W -> C b layers heads H W', C=1)], dim=0)\n",
    "    imgs = rearrange(imgs, 'C B layers heads H W -> B layers heads H W C')    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3207e5-45a5-458e-8100-259f97aa6589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cargpt.models.cilpp import CILpp\n",
    "\n",
    "wandb_model = \"yaak/cargpt/model-uv8xv088:v0\"\n",
    "cilpp = CILpp.load_from_wandb_artifact(name=wandb_model)\n",
    "# cilpp = CILpp.load_from_checkpoint(\"artifacts/model-rc93mcrx:v7/model.ckpt\")\n",
    "cilpp.eval()\n",
    "\n",
    "cfg = OmegaConf.load(\"config/experiment/cilpp.yaml\")\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "\n",
    "# data = datamodule.train_dataloader()\n",
    "data = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324424e-4fe8-4ea9-b884-216c06d4a4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae76fde-8d70-4c78-a6fe-9cd406920d16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "states, frames = get_state_and_frames(cilpp, batch)\n",
    "labels = compute_labels(batch)\n",
    "maps = get_attention_maps(cilpp.transformer_encoder, states)\n",
    "maps = torch.stack(maps)\n",
    "maps = rearrange(maps, 'layers b heads t1 t2 -> b layers heads t1 t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0d0c5-c6b6-45cd-9e4f-3812f215fbca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = merge_frames_with_maps(frames, maps, boost_channel=3, out_height=120, map_weight=1, norm_maps=True, global_norm=True, no_attn_itself=True, avg_heads=False)\n",
    "for i, r in enumerate(out):\n",
    "    print({(k, labels[k][i].item()) for k in sorted(labels.keys())})\n",
    "    fig = get_figure(r, i)\n",
    "    plt.show(fig)\n",
    "    print(\"-\"*60)\n",
    "\n",
    "           \n",
    "out_avg = merge_frames_with_maps(frames, maps, boost_channel=3, out_height=120, map_weight=1, norm_maps=True, global_norm=True, no_attn_itself=True, avg_heads=True)\n",
    "for i, r in enumerate(out_avg):\n",
    "    print({(k, labels[k][i].item()) for k in sorted(labels.keys())})\n",
    "    fig = get_figure(r, i)\n",
    "    plt.show(fig)\n",
    "    print(\"-\"*60)\n",
    "\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
