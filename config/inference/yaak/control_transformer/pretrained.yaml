# @package _global_
---
defaults:
  - /model: yaak/control_transformer/pretrained
  - /datamodule: yaak/predict
  - /paths: yaak/default
  - /logger: yaak/rerun/prediction_pretrain
  - _self_

model:
  hparams_updaters:
    # 1. wrap hparams in DictConfig
    - _target_: omegaconf.DictConfig
      _partial_: true
    # 2. delete unnecessary keys
    - _target_: funcy.del_in
      _partial_: true
      path: ["objectives", "inverse_dynamics", "losses"]
    - _target_: funcy.del_in
      _partial_: true
      path: ["objectives", "forward_dynamics", "losses"]
    - _target_: funcy.del_in
      _partial_: true
      path: ["objectives", "random_masked_hindsight_control", "losses"]
    - _target_: funcy.del_in
      _partial_: true
      path: ["objectives", "memory_extraction", "losses"]
    - _target_: funcy.del_in
      _partial_: true
      path: ["lr_scheduler"]
    - _target_: funcy.set_in
      _partial_: true
      path: [episode_builder, input_transform, _args_, 0, paths, context, waypoints]
      value: [data, "waypoints/xy_normalized"]

trainer:
  _target_: pytorch_lightning.Trainer
  _convert_: all
  accelerator: gpu
  devices: 1
  benchmark: true
  precision: "bf16-mixed"
  logger: false
  callbacks:
    - _target_: rmind.callbacks.RerunPredictionWriter
      write_interval: batch
      logger: ${logger}
