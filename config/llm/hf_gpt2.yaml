have_special_tokens:
  bos: False
  sep: True
  eos: False

model:
  _target_: cargpt.models.llm.HFGPT2
  _recursive_: False
  llm:
    _target_: transformers.GPT2LMHeadModel
    _args_:
      - _target_: transformers.GPT2Config
        vocab_size: ${image_encoder.total_tokens}
        n_positions: ${image_encoder.max_seq_len}
        n_embd: ${embedding_dim}
        n_layer: ${num_layers}
        n_head: ${num_heads}
        n_inner: 512
        activation_function: gelu
