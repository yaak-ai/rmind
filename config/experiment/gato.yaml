# @package _global_
# image_encoder: chose between dall_e or resnet [preferred]
defaults:
  - /paths: default
  - /image_encoder: resnet.yaml
  - override /datamodule: gato.yaml
  - _self_

wandb:
  group: Gato-Image-speed-to-control

batch_size: ${image_encoder.batch_size}
clip_len: ${image_encoder.clip_len}
num_special_tokens: 3  # for special tokens SOS, SEP, OES
num_continous_tokens: 1024 # real values -> bin
num_discrete_tokens: 3 # only turn signals (left, right, none)
num_tokens_total: ${image_encoder.total_tokens} # ${num_special_tokens} + 4 x ${num_continous_tokens} + ${num_discrete_tokens} + ${image_encoder.tokens}
metadata_keys: ["VehicleMotion_speed", "VehicleState_turn_signal"]
speed_range: [0.0, 80.0]
action_keys:
  - "VehicleMotion_gas_pedal_normalized"
  - "VehicleMotion_brake_pedal_normalized"
  - "VehicleMotion_steering_angle_normalized"
tokens_shift: # Shifting continous and discrete tokens since embeddings are shared
  VehicleMotion_speed: 3
  VehicleMotion_gas_pedal_normalized: 1027
  VehicleMotion_brake_pedal_normalized: 2501
  VehicleMotion_steering_angle_normalized: 3075
  VehicleState_turn_signal: 4099
  ImageEncoder: ${image_encoder.tokens_shift}

embedding_dim: 512
dalle_encoder_weights: pretrained/dalle/encoder.pkl

model:
  _target_: cargpt.models.gato.Gato
  _recursive_: False

  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-4
    weight_decay: 0

  image_embedding: ${image_encoder.backbone}
  image_tokens: ${image_encoder.tokenizer}

  sensor_tokenizers:
    _target_: torch.nn.ModuleDict
    modules:
      VehicleMotion_speed:
        _target_: torch.nn.Sequential
        _args_:
          - _target_: cargpt.norm.MinMaxScaler
            in_range: ${speed_range}
            out_range: [0.0, 1.0]
          - _target_: torchaudio.transforms.MuLawEncoding
            quantization_channels: ${num_continous_tokens}
      VehicleMotion_gas_pedal_normalized:
        _target_: torchaudio.transforms.MuLawEncoding
        quantization_channels: ${num_continous_tokens}
      VehicleMotion_brake_pedal_normalized:
        _target_: torchaudio.transforms.MuLawEncoding
        quantization_channels: ${num_continous_tokens}
      VehicleMotion_steering_angle_normalized:
        _target_: torchaudio.transforms.MuLawEncoding
        quantization_channels: ${num_continous_tokens}
      VehicleState_turn_signal:
        _target_: torch.nn.Identity

  sensor_detokenization:
    _target_: torch.nn.ModuleDict
    modules:
      VehicleMotion_speed:
        _target_: torch.nn.Sequential
        _args_:
          - _target_: torchaudio.transforms.MuLawDecoding
            quantization_channels: ${num_continous_tokens}
          - _target_: cargpt.norm.Clamp
            min_value: 0.0
            max_value: 1.0
          - _target_: cargpt.norm.MinMaxScaler
            in_range: [0.0, 1.0]
            out_range: ${speed_range}
      VehicleMotion_gas_pedal_normalized:
        _target_: torch.nn.Sequential
        _args_:
          - _target_: torchaudio.transforms.MuLawDecoding
            quantization_channels: ${num_continous_tokens}
          - _target_: cargpt.norm.Clamp
            min_value: 0.0
            max_value: 1.0
      VehicleMotion_brake_pedal_normalized:
        _target_: torch.nn.Sequential
        _args_:
          - _target_: torchaudio.transforms.MuLawDecoding
            quantization_channels: ${num_continous_tokens}
          - _target_: cargpt.norm.Clamp
            min_value: 0.0
            max_value: 1.0
      VehicleMotion_steering_angle_normalized:
        _target_: torch.nn.Sequential
        _args_:
          - _target_: torchaudio.transforms.MuLawDecoding
            quantization_channels: ${num_continous_tokens}
          - _target_: cargpt.norm.Clamp
            min_value: -1.0
            max_value: 1.0
      VehicleState_turn_signal:
        _target_: torch.nn.Identity

  sensor_embedding:
    _target_: torch.nn.Embedding
    num_embeddings: ${num_tokens_total}
    embedding_dim: ${embedding_dim}

  position_encoding:
    patch_row:
      _target_: torch.nn.Embedding
      num_embeddings: ${image_encoder.patch_row_tokens}
      embedding_dim: ${embedding_dim}
    patch_col:
      _target_: torch.nn.Embedding
      num_embeddings: ${image_encoder.patch_col_tokens}
      embedding_dim: ${embedding_dim}
    local:
      _target_: torch.nn.Embedding
      num_embeddings: ${image_encoder.local_position_tokens}
      embedding_dim: ${embedding_dim}
    global_pos:
      _target_: torch.nn.Embedding
      num_embeddings: ${clip_len}
      embedding_dim: ${embedding_dim}
    action:
      _target_: torch.nn.Embedding
      num_embeddings: 1
      embedding_dim: ${embedding_dim}

  attention_mask:
    _target_: cargpt.models.gato.Gato.causal_attention_mask
    _args_:
      - ${image_encoder.patch_row_tokens}
      - ${image_encoder.patch_col_tokens}
      - ${metadata_keys}
      - ${action_keys}
      - ${clip_len}
    _partial_: True

  gpt:
    _target_: torch.nn.TransformerEncoder
    num_layers: 8
    encoder_layer:
      _target_: cargpt.models.gato.TransformerEncoderLayerGEGLU
      d_model: ${embedding_dim}
      dim_feedforward: 512
      nhead: 4
      dropout: 0.1
      batch_first: True
      norm_first: True
  classifier:
    _target_: torch.nn.Linear
    in_features: ${embedding_dim}
    out_features: ${num_tokens_total}
  regressor:
    _target_: torch.nn.Linear
    in_features: ${embedding_dim}
    out_features: 1

  diff:
    _target_: cargpt.models.losses.DetokenizedL1
    image_tokens_start: ${image_encoder.tokens_shift}

  loss:
    weights:
      categorical: 1.0
      l1: 0
      image: ${image_encoder.loss.weights.ImageEncoder}
      ignore_image_tokens: ${image_encoder.ignore_image_tokens}
    categorical:
      _target_: torch.nn.CrossEntropyLoss
      ignore_index: -1
      reduction: mean
    l1:
      _target_: torch.nn.L1Loss
      reduction: none

  metadata_keys: ${metadata_keys}
  action_keys: ${action_keys}
  ignore_image_tokens: ${image_encoder.ignore_image_tokens}
  special_tokens:
    bos: 0
    sep: 1
    eos: 2
  tokens_shift: ${tokens_shift}
  masks:
    image: ${image_encoder.tokens_mask}
    metadata: 1
    action: 1
  log:
    validation:
      outputs: true

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: [0]
  benchmark: true
  max_steps: 100000
  log_every_n_steps: 1

  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    log_model: all

  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      save_weights_only: True
      save_on_train_epoch_end: True
      monitor: train/loss

datamodule:
  train:
    batch_size: ${batch_size}
    dataset:
      config:
        data:
          metadata:
            filter: VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]}
        samples:
          clips:
            length: ${clip_len}
            stride: 10
            step: 10

  val:
    batch_size: ${batch_size}
    dataset:
      config:
        data:
          metadata:
            filter: VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]}
        samples:
          clips:
            length: ${clip_len}
            stride: 10
            step: 10
