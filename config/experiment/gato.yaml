# @package _global_
# image_encoder: chose between dall_e or resnet [preferred]
# llm: chose between transformer_encoder or hf_gpt2 [preferred]
defaults:
  - /paths: default
  - /image_encoder: resnet.yaml
  - /sensor_encoder: mu_law.yaml
  - /llm: pytorch_gpt2.yaml
  - override /datamodule: gato.yaml
  - _self_

wandb:
  group: Gato-Image-speed-to-control
  dir: ./wandb-runs

num_special_tokens: 3  # for special tokens SOS, SEP, OES
num_continous_tokens: 1024 # real values -> bin
num_discrete_tokens: 3 # only turn signals (left, right, none)
num_tokens_total: ${image_encoder.total_tokens} # ${num_special_tokens} + 4 x ${num_continous_tokens} + ${num_discrete_tokens} + ${image_encoder.tokens}
metadata_keys: ["VehicleMotion_speed", "VehicleState_turn_signal"]
speed_range: ${image_encoder.speed_range}
action_keys:
  - "VehicleMotion_gas_pedal_normalized"
  - "VehicleMotion_brake_pedal_normalized"
  - "VehicleMotion_steering_angle_normalized"
tokens_shift: # Shifting continous and discrete tokens since embeddings are shared
  VehicleMotion_speed: 3
  VehicleMotion_gas_pedal_normalized: 1027
  VehicleMotion_brake_pedal_normalized: 2501
  VehicleMotion_steering_angle_normalized: 3075
  VehicleState_turn_signal: 4099
  ImageEncoder: ${image_encoder.tokens_shift}

embedding_dim: 512
dalle_encoder_weights: pretrained/dalle/encoder.pkl

model:
  _target_: cargpt.models.gato.Gato
  _recursive_: False

  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-4
    weight_decay: 0

  image_embedding: ${image_encoder.backbone}
  image_tokens: ${image_encoder.tokenizer}
  sensor_embedding: ${sensor_encoder.sensor_embedding}
  sensor_tokenizers: ${sensor_encoder.sensor_tokenizers}
  sensor_detokenization: ${sensor_encoder.sensor_detokenization}

  # https://arxiv.org/pdf/1812.03079.pdf
  sensor_dropout:
    _target_: cargpt.models.encoding.SenorDropout
    prob: 1.0

  position_encoding:
    patch_row:
      _target_: torch.nn.Embedding
      num_embeddings: ${image_encoder.patch_row_tokens}
      embedding_dim: ${embedding_dim}
    patch_col:
      _target_: torch.nn.Embedding
      num_embeddings: ${image_encoder.patch_col_tokens}
      embedding_dim: ${embedding_dim}
    local:
      _target_: torch.nn.Embedding
      num_embeddings: ${image_encoder.local_position_tokens}
      embedding_dim: ${embedding_dim}
    global_pos:
      _target_: torch.nn.Embedding
      num_embeddings: ${image_encoder.clip_len}
      embedding_dim: ${embedding_dim}
    action:
      _target_: torch.nn.Embedding
      num_embeddings: 1
      embedding_dim: ${embedding_dim}

  attention_mask:
    _target_: cargpt.models.gato.Gato.causal_attention_mask
    _args_:
      - ${image_encoder.patch_row_tokens}
      - ${image_encoder.patch_col_tokens}
      - ${metadata_keys}
      - ${action_keys}
      - ${image_encoder.clip_len}
    _partial_: True

  gpt: ${llm.model}

  regressor:
    _target_: torch.nn.Linear
    in_features: ${embedding_dim}
    out_features: 1

  diff:
    _target_: cargpt.models.losses.DetokenizedL1
    image_tokens_start: ${image_encoder.tokens_shift}

  loss:
    weights:
      categorical: 1.0
      l1: 0
      image: ${image_encoder.loss.weights.ImageEncoder}
      ignore_image_tokens: ${image_encoder.ignore_image_tokens}
    l1:
      _target_: torch.nn.L1Loss
      reduction: none

  metadata_keys: ${metadata_keys}
  action_keys: ${action_keys}
  ignore_image_tokens: ${image_encoder.ignore_image_tokens}
  special_tokens:
    bos: 0
    sep: 1
    eos: 2
  tokens_shift: ${tokens_shift}
  have_position_encoding: ${llm.have_position_encoding}
  have_special_tokens: ${llm.have_special_tokens}
  masks:
    image: ${image_encoder.tokens_mask}
    metadata: 1
    action: 1
  log:
    validation:
      outputs: true
      attention_maps:
        layer: -1
        rows: ${image_encoder.patch_row_tokens}
        cols: ${image_encoder.patch_col_tokens}
        imgs_mean: ${image_encoder.mean}
        imgs_std: ${image_encoder.std}
        norm: max  # softmax or max
        strength: 1.0
        reverse: false
        color: [255, 0, 0]  # RGB
        log_freq: 50
    training:
      attention_maps:
        layer: -1
        rows: ${image_encoder.patch_row_tokens}
        cols: ${image_encoder.patch_col_tokens}
        imgs_mean: ${image_encoder.mean}
        imgs_std: ${image_encoder.std}
        norm: max  # softmax or max
        strength: 1.0
        reverse: false
        color: [255, 0, 0]  # RGB
        log_freq: 100

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: [0]
  benchmark: true
  max_steps: 100000
  log_every_n_steps: 1
  precision: 16

  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    log_model: all

  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      save_weights_only: False
      save_on_train_epoch_end: True
      monitor: train/loss

datamodule:
  train:
    batch_size: ${image_encoder.batch_size}
    dataset:
      config:
        data:
          metadata:
            filter: VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]}
        samples:
          clips:
            length: ${image_encoder.clip_len}
            stride: 10
            step: 10
            frame_mask: ${image_encoder.frame_mask}

  val:
    batch_size: ${image_encoder.batch_size}
    dataset:
      config:
        data:
          metadata:
            filter: VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]}
        samples:
          clips:
            length: ${image_encoder.clip_len}
            stride: 10
            step: 10
            frame_mask: ${image_encoder.frame_mask}
