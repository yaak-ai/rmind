# @package _global_

defaults:
  - /paths: default
  - override /datamodule: gato.yaml
  - _self_

wandb:
  group: Gato-Image-speed-to-control

batch_size: 8
clip_len: 6
num_special_tokens: 3  # for special tokens SOS, SEP, OES
num_continous_tokens: 1024 # real values -> bin
num_discrete_tokens: 3 # only turn signals (left, right, none)
num_tokens_total: 4102 # ${num_special_tokens} + 4 x ${num_continous_tokens} + ${num_discrete_tokens}
metadata_keys: ["VehicleMotion_speed"]
speed_range: [0.0, 80.0]
action_keys:
  - "VehicleMotion_gas_pedal_normalized"
  - "VehicleMotion_brake_pedal_normalized"
  - "VehicleMotion_steering_angle_normalized"
  - "VehicleState_turn_signal"
tokens_shift: # Shifting continous and discrete tokens since embeddings are shared
  VehicleMotion_speed: 3
  VehicleMotion_gas_pedal_normalized: 1027
  VehicleMotion_brake_pedal_normalized: 2501
  VehicleMotion_steering_angle_normalized: 3075
  VehicleState_turn_signal: 4099
patch_row_bins: 10
patch_col_bins: 18
embedding_dim: 512

model:
  _target_: cargpt.models.gato.Gato
  _recursive_: False

  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-4
    weight_decay: 0

  image_embedding:
    _target_: cargpt.models.encoding.ResnetBackbone
    resnet:
      _target_: torchvision.models.resnet18
      weights: IMAGENET1K_V1
    freeze: True

  sensor_tokenizers:
    _target_: torch.nn.ModuleDict
    modules:
      VehicleMotion_speed:
        _target_: torch.nn.Sequential
        _args_:
          - _target_: cargpt.norm.MinMaxScaler
            in_range: ${speed_range}
            out_range: [0.0, 1.0]
          - _target_: torchaudio.transforms.MuLawEncoding
            quantization_channels: ${num_continous_tokens}
      VehicleMotion_gas_pedal_normalized:
        _target_: torchaudio.transforms.MuLawEncoding
        quantization_channels: ${num_continous_tokens}
      VehicleMotion_brake_pedal_normalized:
        _target_: torchaudio.transforms.MuLawEncoding
        quantization_channels: ${num_continous_tokens}
      VehicleMotion_steering_angle_normalized:
        _target_: torchaudio.transforms.MuLawEncoding
        quantization_channels: ${num_continous_tokens}
      VehicleState_turn_signal:
        _target_: torch.nn.Identity

  sensor_embedding:
    _target_: torch.nn.Embedding
    num_embeddings: ${num_tokens_total}
    embedding_dim: ${embedding_dim}

  position_encoding:
    patch_row:
      _target_: torch.nn.Embedding
      num_embeddings: ${patch_row_bins}
      embedding_dim: ${embedding_dim}
    patch_col:
      _target_: torch.nn.Embedding
      num_embeddings: ${patch_col_bins}
      embedding_dim: ${embedding_dim}
    local:
      _target_: torch.nn.Embedding
      num_embeddings: ${clip_len}
      embedding_dim: ${embedding_dim}
    action:
      _target_: torch.nn.Embedding
      num_embeddings: 1
      embedding_dim: ${embedding_dim}

  gpt:
    _target_: torch.nn.TransformerEncoder
    num_layers: 8
    encoder_layer:
      _target_: cargpt.models.gato.TransformerEncoderLayerGEGLU
      d_model: ${embedding_dim}
      dim_feedforward: 3072
      nhead: 16
      dropout: 0.1
      batch_first: True
      norm_first: True
  classifier:
    _target_: torch.nn.Linear
    in_features: ${embedding_dim}
    out_features: ${num_tokens_total}

  metadata_keys: ${metadata_keys}
  action_keys: ${action_keys}
  special_tokens:
    bos: 0
    sep: 1
    eos: 2
  tokens_shift: ${tokens_shift}
  masks:
    image: -1 # to ignore_index in torch.nn.functional.cross_entropy
    metadata: 1
    action: 1
  log:
    validation:
      outputs: true

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: [0]
  benchmark: true
  max_steps: 100000
  log_every_n_steps: 1

  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    log_model: all

  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      save_weights_only: True
      save_on_train_epoch_end: True
      monitor: train/loss

datamodule:
  train:
    batch_size: ${batch_size}
    dataset:
      config:
        data:
          metadata:
            filter: VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]}
        samples:
          clips:
            length: ${clip_len}
            stride: 10
            step: 10

  val:
    batch_size: ${batch_size}
    dataset:
      config:
        data:
          metadata:
            filter: VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]}
        samples:
          clips:
            length: ${clip_len}
            stride: 10
            step: 10
