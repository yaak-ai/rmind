# @package _global_
# image_encoder: chose between dall_e or resnet [preferred]
# llm: chose between transformer_encoder or hf_gpt2 [preferred]
defaults:
  - /paths: default
  - /image_encoder: resnet.yaml
  - /sensor_encoder: mu_law.yaml
  - /llm: pytorch_gpt2.yaml
  - override /datamodule: gato.yaml
  - _self_

wandb:
  group: Gato-Image-speed-to-control
  dir: ./wandb-runs

num_special_tokens: 3  # for special tokens MASK, SEP, OES
num_continous_tokens: 1024 # real values -> bin
num_discrete_tokens: 3 # only turn signals (left, right, none)
num_tokens_total: ${image_encoder.total_tokens} # ${num_special_tokens} + 4 x ${num_continous_tokens} + ${num_discrete_tokens} + ${image_encoder.tokens}
metadata_keys: ["VehicleMotion_speed", "VehicleState_turn_signal"]
speed_range: ${image_encoder.speed_range}
action_keys:
  - [gas_minus_brake, "VehicleMotion_gas_pedal_normalized", "VehicleMotion_brake_pedal_normalized"]
  - "VehicleMotion_steering_angle_normalized"
tokens_shift: # Shifting continous and discrete tokens since embeddings are shared
  VehicleMotion_speed: ${num_special_tokens}
  gas_minus_brake: "${eval:'${num_special_tokens} + 0 * ${num_continous_tokens}'}"
  VehicleMotion_steering_angle_normalized: "${eval:'${num_special_tokens} + 0 * ${num_continous_tokens}'}"
  VehicleState_turn_signal: "${eval:'${num_special_tokens} + 1 * ${num_continous_tokens}'}"
  ImageEncoder: ${image_encoder.tokens_shift}

objective: pretraining

num_heads: 8
num_layers: 24
embedding_dim: 512
dalle_encoder_weights: pretrained/dalle/encoder.pkl

model:
  _target_: cargpt.models.gato.Gato
  _recursive_: False

  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-4
    weight_decay: 0

  image_embedding: ${image_encoder.backbone}
  image_tokens: ${image_encoder.tokenizer}
  sensor_embedding: ${sensor_encoder.sensor_embedding}
  sensor_tokenizers: ${sensor_encoder.sensor_tokenizers}
  sensor_detokenization: ${sensor_encoder.sensor_detokenization}

  # https://arxiv.org/pdf/1812.03079.pdf
  # https://arxiv.org/pdf/1905.11979.pdf
  sensor_dropout:
    _target_: cargpt.models.encoding.SenorDropout
    prob: 1.0

  position_encoding:
    patch_row:
      _target_: torch.nn.Embedding
      num_embeddings: ${image_encoder.patch_row_tokens}
      embedding_dim: ${embedding_dim}
    patch_col:
      _target_: torch.nn.Embedding
      num_embeddings: ${image_encoder.patch_col_tokens}
      embedding_dim: ${embedding_dim}
    local:
      _target_: torch.nn.Embedding
      num_embeddings: ${image_encoder.local_position_tokens}
      embedding_dim: ${embedding_dim}
    global_pos:
      _target_: torch.nn.Embedding
      num_embeddings: ${image_encoder.clip_len}
      embedding_dim: ${embedding_dim}
    action:
      _target_: torch.nn.Embedding
      num_embeddings: 1
      embedding_dim: ${embedding_dim}

  attention_mask:
    gato:
      _target_: cargpt.models.gato.Gato.causal_attention_mask
      _args_:
        - ${image_encoder.patch_row_tokens}
        - ${image_encoder.patch_col_tokens}
        - ${metadata_keys}
        - ${action_keys}
        - ${image_encoder.clip_len}
      _partial_: True
    forward_dynamics:
      _target_: cargpt.models.gato.Gato.causal_attention_mask
      _args_:
        - ${image_encoder.patch_row_tokens}
        - ${image_encoder.patch_col_tokens}
        - ${metadata_keys}
        - ${action_keys}
        - ${image_encoder.clip_len}
      _partial_: True
    inverse_dynamics:
      _target_: cargpt.models.gato.Gato.block_all_sensor_and_image_from_sensor_attention_mask
      _args_:
        - ${image_encoder.patch_row_tokens}
        - ${image_encoder.patch_col_tokens}
        - ${metadata_keys}
        - ${action_keys}
        - ${image_encoder.clip_len}
      _partial_: True
    hindsight_control:
      _target_: cargpt.models.gato.Gato.full_attention_mask
      _args_:
        - ${image_encoder.patch_row_tokens}
        - ${image_encoder.patch_col_tokens}
        - ${metadata_keys}
        - ${action_keys}
        - ${image_encoder.clip_len}
      _partial_: True

  gpt: ${llm.model}
  heads:
    _target_: torch.nn.ModuleDict
    modules:
      gato:
        _target_: torch.nn.Linear
        in_features: ${embedding_dim}
        out_features: ${image_encoder.total_tokens}
        bias: False
      forwad_dynamics:
        _target_: torch.nn.Linear
        in_features: "${eval:'2 * ${embedding_dim}'}"
        out_features: ${embedding_dim}
        bias: False
      inverse_dynamics:
        _target_: torch.nn.Linear
        in_features: "${eval:'2 * ${embedding_dim}'}"
        out_features: ${image_encoder.total_tokens}
        bias: False
      hindsight_control:
        _target_: torch.nn.Linear
        in_features: ${embedding_dim}
        out_features: ${image_encoder.total_tokens}
        bias: False
  loss:
    ce:
      _target_: torch.nn.CrossEntropyLoss
      ignore_index: -100
      reduction: mean
    mse:
      _target_: torch.nn.MSELoss
      reduction: mean

  regressor:
    _target_: torch.nn.Linear
    in_features: ${embedding_dim}
    out_features: 1

  diff:
    _target_: cargpt.models.losses.DetokenizedL1
    image_tokens_start: ${image_encoder.tokens_shift}

  loss:
    weights:
      categorical: 1.0
      l1: 0
    l1:
      _target_: torch.nn.L1Loss
      reduction: none

  metadata_keys: ${metadata_keys}
  action_keys: ${action_keys}
  clip_len: ${image_encoder.clip_len}
  special_tokens:
    mask: 0
    sep: 1
    eos: 2
  tokens_shift: ${tokens_shift}
  have_position_encoding: ${llm.have_position_encoding}
  have_special_tokens: ${llm.have_special_tokens}
  masks:
    image: ${image_encoder.tokens_mask}
    metadata: 1
    action: 1
  objective: ${objective}
  pretraining:
    - forwad_dynamics
    - inverse_dynamics
    - hindsight_control
  finetuning:
    - gato

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: [0]
  benchmark: true
  max_steps: 100000
  log_every_n_steps: 1

  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    log_model: all

  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      save_weights_only: False
      save_on_train_epoch_end: True
      monitor: train/loss

datamodule:
  train:
    batch_size: ${image_encoder.batch_size}
    dataset:
      config:
        data:
          metadata:
            filter: VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]}
        samples:
          clips:
            length: ${image_encoder.clip_len}
            stride: 10
            step: 10
            frame_mask: ${image_encoder.frame_mask}

  val:
    batch_size: ${image_encoder.batch_size}
    dataset:
      config:
        data:
          metadata:
            filter: VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]}
        samples:
          clips:
            length: ${image_encoder.clip_len}
            stride: 10
            step: 10
            frame_mask: ${image_encoder.frame_mask}
