# @package _global_

defaults:
  - /paths: default
  - /datamodule: default.yaml
  - _self_

# TODO: match paper?
batch_size: 16
clip_len: 6
num_heads: 4
num_layers: 8
embedding_dim: 512

quantization_channels: 1024

speed_range: [5.0, 130.0]

wandb:
  group: smart

model:
  _target_: cargpt.models.control_transformer.ControlTransformer
  _recursive_: false

  episode_builder:
    _target_: cargpt.components.episode.EpisodeBuilder
    _recursive_: true

    timestep:
      _target_: cargpt.components.episode.Timestep.build
      observations:
        - [image, cam_front_left]
        - [continuous, speed]
        - [discrete, turn_signal]
      actions:
        - [continuous, pedal]
        - [continuous, steering_angle]

    transforms:
      _target_: torch.nn.ModuleDict
      modules:
        continuous:
          _target_: torch.nn.ModuleDict
          modules:
            speed:
              _target_: cargpt.components.norm.MinMaxScaler
              in_range: ${speed_range}
              out_range: [-1.0, 1.0]

    tokenizers:
      _target_: torch.nn.ModuleDict
      modules:
        image:
          _target_: torch.nn.Identity

        continuous:
          _target_: torch.nn.Sequential
          _args_:
            - _target_: torchaudio.transforms.MuLawEncoding
              quantization_channels: ${quantization_channels}

            - _target_: einops.layers.torch.Rearrange
              pattern: b t -> b t 1

        discrete:
          _target_: einops.layers.torch.Rearrange
          pattern: b t -> b t 1

    embeddings:
      _target_: torch.nn.ModuleDict
      modules:
        image:
          _target_: torch.nn.Sequential
          _args_:
            - _target_: cargpt.components.resnet.ResnetBackbone
              resnet:
                _target_: torchvision.models.resnet18
                weights: IMAGENET1K_V1
              freeze: True

            - _target_: einops.layers.torch.Rearrange
              pattern: ... c h w -> ... h w c

            - _target_: cargpt.components.position_encoding.PatchPositionEncoding
              num_rows: 10
              num_cols: 18
              embedding_dim: ${embedding_dim}

            - _target_: einops.layers.torch.Rearrange
              pattern: ... h w d -> ... (h w) d

        continuous:
          _target_: torch.nn.Embedding
          num_embeddings: ${quantization_channels}
          embedding_dim: ${embedding_dim}

        discrete:
          _target_: torch.nn.Embedding
          num_embeddings: 3
          embedding_dim: ${embedding_dim}

    position_encoding:
      _target_: torch.nn.ModuleDict
      modules:
        observations:
          _target_: torch.nn.Embedding
          num_embeddings: 182
          embedding_dim: ${embedding_dim}

        actions:
          _target_: torch.nn.Embedding
          num_embeddings: 1
          embedding_dim: ${embedding_dim}

        timestep:
          _target_: torch.nn.Embedding
          num_embeddings: ${clip_len}
          embedding_dim: ${embedding_dim}

  encoder:
    _target_: cargpt.components.llm.xFormerEncoder
    config:
      _target_: xformers.factory.xFormerEncoderConfig
      reversible: True
      num_layers: ${num_layers}
      dim_model: ${embedding_dim}
      residual_norm_style: pre
      multi_head_config:
        dim_model: ${embedding_dim}
        num_heads: ${num_heads}
        residual_dropout: 0.1
        attention:
          name: scaled_dot_product
          dropout: 0.1
          causal: False
      feedforward_config:
        name: MLPGLU
        dropout: 0.1
        activation: gelu
        hidden_layer_multiplier: 1

  objectives:
    _target_: torch.nn.ModuleDict
    modules:
      forward_dynamics:
        _target_: cargpt.models.control_transformer.ForwardDynamicsPredictionObjective
        head:
          _target_: torch.nn.Linear
          in_features:
            _target_: operator.mul
            _args_:
              - 2
              - ${embedding_dim}

          out_features: ${embedding_dim}
          bias: True

        loss:
          _target_: torch.nn.MSELoss

      # inverse_dynamics:
      #   _target_: cargpt.models.control_transformer.InverseDynamicsPredictionObjective
      #   heads:
      #     _target_: torch.nn.ModuleDict
      #     modules:
      #       continuous:
      #         _target_: torch.nn.ModuleDict
      #         modules:
      #           pedal:
      #             _target_: torch.nn.Linear
      #             in_features:
      #               _target_: operator.mul
      #               _args_:
      #                 - 2
      #                 - ${embedding_dim}
      #             out_features: ${model.episode_builder.embeddings.modules.continuous.num_embeddings}
      #             bias: False
      #
      #           steering_angle:
      #             _target_: torch.nn.Linear
      #             in_features:
      #               _target_: operator.mul
      #               _args_:
      #                 - 2
      #                 - ${embedding_dim}
      #             out_features: ${model.episode_builder.embeddings.modules.continuous.num_embeddings}
      #             bias: False
      #
      #   loss:
      #     _target_: torch.nn.CrossEntropyLoss
      #     reduction: mean
      #
      #   detokenizers:
      #     _target_: torch.nn.ModuleDict
      #     modules:
      #       continuous:
      #         _target_: torchaudio.transforms.MuLawDecoding
      #         quantization_channels: ${quantization_channels}

      random_masked_hindsight_control:
        _target_: cargpt.models.control_transformer.RandomMaskedHindsightControlObjective
        heads:
          _target_: torch.nn.ModuleDict
          modules:
            continuous:
              _target_: torch.nn.ModuleDict
              modules:
                pedal:
                  _target_: torch.nn.Linear
                  in_features: ${embedding_dim}
                  out_features: ${model.episode_builder.embeddings.modules.continuous.num_embeddings}
                  bias: True

                steering_angle:
                  _target_: torch.nn.Linear
                  in_features: ${embedding_dim}
                  out_features: ${model.episode_builder.embeddings.modules.continuous.num_embeddings}
                  bias: True

        loss:
          _target_: torch.nn.CrossEntropyLoss
          reduction: mean

  # TODO: match paper?
  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-4
    weight_decay: 0

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: [0]
  benchmark: true
  max_epochs: -1
  log_every_n_steps: 100

  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    log_model: all

  callbacks:
    - _target_: cargpt.callbacks.model_summary.ModelSummary
      depth: 5

    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: train/loss/total
      save_on_train_epoch_end: True

datamodule:
  train:
    batch_size: ${batch_size}
    num_workers: 16
    dataset:
      config:
        data:
          metadata:
            select:
              - message: ImageMetadata
                fields:
                  - name: frame_idx
                  - name: camera_name

              - message: VehicleMotion
                fields:
                  - name: speed
                  - name: steering_angle_normalized
                  - name: gas_pedal_normalized
                  - name: brake_pedal_normalized

              - message: VehicleState
                fields:
                  - name: turn_signal

            filter: >
              VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]}
              and VehicleMotion_gas_pedal_normalized between 0.0 and 1.0
              and VehicleMotion_brake_pedal_normalized between 0.0 and 1.0
              and VehicleMotion_steering_angle_normalized between -1.0 and 1.0
              and (VehicleMotion_gas_pedal_normalized > 0.0 or VehicleMotion_brake_pedal_normalized > 0.0)

        samples:
          clips:
            length: ${clip_len}
            stride: 10
            step: 10

          transforms:
            - _target_: yaak_datasets.transforms.Crop
              offsets: [2, 2, 0, 0]

            - _target_: yaak_datasets.transforms.Normalize
              # ImageNet stats
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]

  val:
    batch_size: ${batch_size}
    num_workers: ${datamodule.train.num_workers}
    dataset:
      config:
        data:
          metadata: ${datamodule.train.dataset.config.data.metadata}
        samples: ${datamodule.train.dataset.config.samples}
