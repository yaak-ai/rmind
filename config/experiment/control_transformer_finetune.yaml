# @package _global_

defaults:
  - /paths: default
  - /datamodule: default
  - /trainer: default
  - _self_

batch_size: 64
clip_len: 6
embedding_dim: 512


wandb:
  group: smart

vars:
  data:
    train:
      batch_size: 32
      clip_length: 6

    val:
      batch_size: 32
      clip_length: 12

    test:
      clip_length: 24

  encoder:
    num_heads: 4
    num_layers: 8

  embedding_dim: 512

model:
  # _target_: cargpt.models.control_transformer.ControlTransformer.load_from_wandb_artifact
  # artifact: ???
  _target_: cargpt.models.control_transformer.ControlTransformer.load_from_checkpoint
  checkpoint_path: ???
  filename: model.ckpt
  strict: false
  hparams_updaters:
    # wrap hparams in DictConfig
    - _target_: omegaconf.DictConfig
      _partial_: true

    # override objectives to only use copycat/policy
    - _target_: omegaconf.OmegaConf.update
      _partial_: true
      _recursive_: false
      merge: false
      key: objectives
      value:
        _target_: cargpt.utils.ModuleDict
        copycat:
          _target_: cargpt.components.objectives.CopycatObjective
          policy:
            _target_: cargpt.components.objectives.copycat.PolicyStream
            branches:
              _target_: cargpt.utils.ModuleDict
              continuous:
                gas_pedal:
                  _target_: torch.nn.ModuleList
                  modules:
                    - _target_: cargpt.components.Branch
                      heads:
                        _target_: cargpt.components.modules.SequentialModule
                        layer_dims: 
                          - _target_: operator.mul
                            _args_:
                              - 2
                              - ${vars.embedding_dim}
                          - 1024
                          - 512
                          - 512
                      losses:
                        _target_: cargpt.components.loss.LogitBiasCrossEntropyLoss

                brake_pedal:
                  _target_: torch.nn.ModuleList
                  modules:
                    - _target_: cargpt.components.Branch
                      heads:
                        _target_: cargpt.components.modules.SequentialModule
                        layer_dims: 
                          - _target_: operator.mul
                            _args_:
                              - 2
                              - ${vars.embedding_dim}
                          - 1024
                          - 512
                          - 512
                      losses:
                        _target_: cargpt.components.loss.LogitBiasCrossEntropyLoss

                steering_angle:
                  _target_: torch.nn.ModuleList
                  modules:
                    - _target_: cargpt.components.Branch
                      heads:
                        _target_: cargpt.components.modules.SequentialModule
                        layer_dims: 
                          - _target_: operator.mul
                            _args_:
                              - 2
                              - ${vars.embedding_dim}
                          - 1024
                          - 512
                          - 512
                      losses:
                        _target_: cargpt.components.loss.LogitBiasCrossEntropyLoss
                      

    # freeze episode builder (embeddings etc)
    - _target_: omegaconf.OmegaConf.update
      _partial_: true
      _recursive_: false
      merge: true
      key: episode_builder.freeze
      value: true

    # freeze encoder
    - _target_: omegaconf.OmegaConf.update
      _partial_: true
      _recursive_: false
      merge: true
      key: encoder.freeze
      value: true

    # remove objective scheduler since we only have a single objective
    - _target_: omegaconf.OmegaConf.update
      _partial_: true
      _recursive_: false
      merge: false
      key: objective_scheduler
      value: null

    # for some reason lr_scheduler.scheduler isn't checkpointed? re-add
    - _target_: omegaconf.OmegaConf.update
      _partial_: true
      _recursive_: false
      merge: false
      key: lr_scheduler
      value:
        interval: step
        scheduler:
          _target_: transformers.get_cosine_schedule_with_warmup
          num_warmup_steps: 25000
          num_training_steps: 250000

