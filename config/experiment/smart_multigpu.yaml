# @package _global_

defaults:
  - smart
  - _self_

batch_size: 32
  
wandb:
  mode: offline
  notes: "multigpu"

trainer:
  devices: [0,1]
  sync_batchnorm: True
  strategy:
    _target_: pytorch_lightning.strategies.DDPStrategy
    static_graph: True
  
