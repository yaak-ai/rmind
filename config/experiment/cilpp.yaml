# @package _global_

defaults:
  - /paths: default.yaml
  - override /datamodule: default.yaml
  - _self_

batch_size: 32
clip_len: 1
seq_len: 180

d_model: 512

speed_range: [8.0, 40.0]

model: 
  _target_: cargpt.models.cilpp.CILpp
  _recursive_: False

  state_embedding:
    _target_: torch.nn.ModuleDict
    modules:
      frame:
        _target_: torch.nn.ModuleDict
        modules:
          backbone:
            _target_: cargpt.models.encoding.ResnetBackbone
            resnet:
              _target_: torchvision.models.resnet34
              weights: IMAGENET1K_V1
            freeze: True

          depth:
            _target_: cargpt.models.cilpp.DepthFrameEncoder
            disp_net:
              _target_: builtins.getattr
              _args_:
                - _target_: deephouse.models.depth.DepthModule.load_from_wandb_artifact
                  _args_: ['yaak/self-supervised-depth/model-usdyi67i:v0']
                - disp_net

            point_positional_encoder:
              _target_: cargpt.models.encoding.PointPositionalEncoder3D
              channels: ${d_model}
              temperature: 1e4

            target_shape: [10, 18]

      speed:
        _target_: torch.nn.Sequential
        _args_:
          - _target_: cargpt.norm.MinMaxScaler
            in_range: ${speed_range}
            out_range: [0.0, 1.0]

          - _target_: torch.nn.Linear
            in_features: 1
            out_features: ${d_model}

      turn_signal: 
        _target_: torch.nn.Linear
        in_features: 3 # length of VehicleState::TurnSignal enum
        out_features: ${d_model}

      position:
        _target_: cargpt.models.encoding.LearnablePositionalEmbedding1D
        seq_len: ${seq_len}
        embedding_dim: ${d_model}


  transformer_encoder:
    _target_: torch.nn.TransformerEncoder
    num_layers: 4
    encoder_layer:
      _target_: torch.nn.TransformerEncoderLayer
      d_model: ${d_model}
      dim_feedforward: ${d_model}
      nhead: 4
      dropout: 0
      activation: 'relu'
      batch_first: True

  action_prediction:
    _target_: torch.nn.Sequential
    _args_:
      - _target_: einops.layers.torch.Reduce
        pattern: b s c -> b 1 c
        reduction: mean

      - _target_: torchvision.ops.MLP
        in_channels: ${d_model}
        hidden_channels:
          - ${d_model}
          - 256
          - 2
        activation_layer: 
          _target_: torch.nn.ReLU  
          _partial_: True

  loss:
    acceleration:
      _target_: torch.nn.L1Loss

    steering_angle:
      _target_: torch.nn.L1Loss

    weights:
      acceleration: 0.5
      steering_angle: 0.5

  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-4
    weight_decay: 0

  lr_scheduler: null

  log:
    validation:
      frames: true
      outputs: true

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: [0]
  benchmark: true
  max_steps: 100000
  log_every_n_steps: 10

  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    log_model: True

  callbacks:
    - _target_: cargpt.callbacks.ModelSummary
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      save_weights_only: True
      save_on_train_epoch_end: True
      monitor: train/loss/total


_speed_range: ${model.state_embedding.modules.speed._args_[0]}

datamodule:
  train:
    batch_size: ${batch_size}
    dataset:
      config:
        data:
          metadata:
            filter: VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]} 

        samples:
          clips:
            length: ${clip_len}
            stride: 5
            step: 5

  val:
    batch_size: ${batch_size}
    dataset:
      config:
        data:
          metadata:
            filter: VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]} 

        samples:
          clips:
            length: ${clip_len}
            stride: 5
            step: 5
