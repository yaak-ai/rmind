# @package _global_

defaults:
  - /paths: default
  - override /datamodule: gato.yaml
  - _self_

batch_size: 16
clip_len: 6
num_heads: 4
num_layers: 8
embedding_dim: 512
quantization_channels: 1024

speed_range: [0.0, 130.0]

wandb:
  group: gato

model:
  _target_: cargpt.models.gato2.Gato
  _recursive_: false

  transforms:
    _target_: torch.nn.ModuleDict
    modules:
      continuous:
        _target_: torch.nn.ModuleDict
        modules:
          speed:
            _target_: cargpt.norm.MinMaxScaler
            in_range: ${speed_range}
            out_range: [-1.0, 1.0]

  tokenizers:
    _target_: torch.nn.ModuleDict
    modules:
      continuous:
        _target_: torchaudio.transforms.MuLawEncoding
        quantization_channels: ${quantization_channels}

      discrete:
        _target_: torch.nn.Identity

  detokenizers:
    _target_: torch.nn.ModuleDict
    modules:
      continuous:
        _target_: torchaudio.transforms.MuLawDecoding
        quantization_channels: ${quantization_channels}

      discrete:
        _target_: torch.nn.Identity

  embeddings:
    _target_: torch.nn.ModuleDict
    modules:
      image:
        _target_: torch.nn.Sequential
        _args_:
          - _target_: cargpt.models.encoding.ResnetBackbone
            resnet:
              _target_: torchvision.models.resnet18
              weights: IMAGENET1K_V1
            freeze: True

          - _target_: einops.layers.torch.Rearrange
            pattern: ... c h w -> ... h w c

          - _target_: cargpt.models.encoding.PatchPositionEncoding
            num_rows: 10
            num_cols: 18
            embedding_dim: ${embedding_dim}

          - _target_: einops.layers.torch.Rearrange
            pattern: ... h w d -> ... (h w) d

      continuous:
        _target_: torch.nn.Embedding
        num_embeddings: ${quantization_channels}
        embedding_dim: ${embedding_dim}

      discrete:
        _target_: torch.nn.Embedding
        num_embeddings: 3
        embedding_dim: ${embedding_dim}

  episode:
    _target_: cargpt.models.gato2.EpisodeConfig
    observations:
      - [image, cam_front_left]
      - [continuous, speed]
      - [discrete, turn_signal]
    actions:
      - [continuous, pedal]
      - [continuous, steering_angle]

  position_encoding:
    _target_: torch.nn.ModuleDict
    modules:
      observations:
        _target_: torch.nn.Embedding
        num_embeddings: 182
        embedding_dim: ${embedding_dim}

      actions:
        _target_: torch.nn.Embedding
        num_embeddings: 1
        embedding_dim: ${embedding_dim}

      episode:
        _target_: torch.nn.Embedding
        num_embeddings: ${clip_len}
        embedding_dim: ${embedding_dim}

  encoder:
    _target_: cargpt.models.llm.xFormerEncoder
    config:
      _target_: xformers.factory.xFormerEncoderConfig
      reversible: True
      num_layers: ${num_layers}
      dim_model: ${embedding_dim}
      residual_norm_style: pre
      multi_head_config:
        dim_model: ${embedding_dim}
        num_heads: ${num_heads}
        residual_dropout: 0.1
        attention:
          name: scaled_dot_product
          dropout: 0.1
          causal: False
      feedforward_config:
        name: MLPGLU
        dropout: 0.1
        activation: gelu
        hidden_layer_multiplier: 1

  decoders:
    _target_: torch.nn.ModuleDict
    modules:
      continuous:
        _target_: torch.nn.Linear
        in_features: ${embedding_dim}
        out_features: ${model.embeddings.modules.continuous.num_embeddings}
        bias: False

      discrete:
        _target_: torch.nn.Linear
        in_features: ${embedding_dim}
        out_features: ${model.embeddings.modules.discrete.num_embeddings}
        bias: False

  loss:
    _target_: torch.nn.CrossEntropyLoss
    ignore_index: -100
    reduction: mean

  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-4
    weight_decay: 0

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: [0]
  benchmark: true
  max_epochs: -1
  log_every_n_steps: 10

  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    log_model: all

  callbacks:
    - _target_: cargpt.callbacks.ModelSummary

    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: train/loss/total
      save_on_train_epoch_end: True

datamodule:
  train:
    batch_size: ${batch_size}
    num_workers: 2
    dataset:
      config:
        data:
          metadata:
            select:
              - message: ImageMetadata
                fields:
                  - name: frame_idx
                  - name: camera_name

              - message: VehicleMotion
                fields:
                  - name: speed
                  - name: steering_angle_normalized
                  - name: gas_pedal_normalized
                  - name: brake_pedal_normalized

              - message: VehicleState
                fields:
                  - name: turn_signal

            filter: >
              VehicleMotion_speed between ${speed_range[0]} and ${speed_range[1]}
              and VehicleMotion_gas_pedal_normalized between 0.0 and 1.0
              and VehicleMotion_brake_pedal_normalized between 0.0 and 1.0
              and VehicleMotion_steering_angle_normalized between -1.0 and 1.0

        samples:
          clips:
            length: ${clip_len}
            stride: 10
            step: 10

          transforms:
            - _target_: yaak_datasets.transforms.Crop
              offsets: [2, 2, 0, 0]

            - _target_: yaak_datasets.transforms.Normalize
              # ImageNet stats
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]

  val:
    batch_size: ${batch_size}
    num_workers: ${datamodule.train.num_workers}
    dataset:
      config:
        data:
          metadata: ${datamodule.train.dataset.config.data.metadata}
        samples: ${datamodule.train.dataset.config.samples}
