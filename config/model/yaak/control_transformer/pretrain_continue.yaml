_target_: rmind.models.control_transformer.ControlTransformer.load_from_wandb_artifact
artifact: ???
filename: model.ckpt
hparams_updaters:
  # wrap hparams in DictConfig
  - _target_: omegaconf.DictConfig
    _partial_: true
  # for some reason lr_scheduler.scheduler isn't checkpointed? re-add
  - _target_: omegaconf.OmegaConf.update
    _partial_: true
    _recursive_: false
    merge: false
    key: lr_scheduler
    value:
      interval: step
      scheduler:
        _target_: transformers.get_cosine_schedule_with_warmup
        num_warmup_steps: 25000
        num_training_steps: 250000
